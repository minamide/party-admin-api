#!/usr/bin/env python3
"""
CSV -> INSERT generator for `census_mesh_2020`.
Usage: python genarate_census_mesh_2020.py --indir <dir> --outfile <file>

This processes all CSV files in the input directory and writes SQL
inserts (one per row) into the output file. Empty or "*" values become NULL.
Existing `key_code` are skipped by using INSERT ... SELECT ... WHERE NOT EXISTS.
"""

import os
import glob
import csv
import sys
import argparse
import io

TARGET_COLS = [
    'key_code','htk_syori','htk_saki','gassan'
]
# add t001101001 .. t001101050
for i in range(1,51):
    TARGET_COLS.append(f"t001101{str(i).zfill(3)}")


def normalize_header_cell(s: str) -> str:
    if s is None:
        return ''
    s = s.strip()
    # lower, replace non-alnum with underscore
    return ''.join([c.lower() if c.isalnum() else '_' for c in s])


def quote_sql(s: str) -> str:
    return "'" + s.replace("'", "''") + "'"


def value_to_sql(col: str, raw: str) -> str:
    if raw is None:
        return 'NULL'
    v = raw.strip()
    if v == '' or v == '*' or v == '\u3000':  # include full-width space
        return 'NULL'
    if col in ('key_code','htk_saki','gassan'):
        return quote_sql(v)
    # numeric column
    # remove commas and spaces
    v2 = v.replace(',', '').strip()
    try:
        # allow integers only
        ival = int(v2)
        return str(ival)
    except Exception:
        return 'NULL'


def build_index_map(header):
    normed = [normalize_header_cell(h) for h in header]
    mapping = {}
    for col in TARGET_COLS:
        # possible header names to match
        candidates = [col, col.upper(), col.replace('_',''), col.replace('_','').upper(), col.replace('t','T')]
        found = None
        for i,h in enumerate(normed):
            if h == col:
                found = i
                break
        if found is None:
            # try matching ignoring underscores
            for i,h in enumerate(normed):
                if h.replace('_','') == col.replace('_',''):
                    found = i
                    break
        if found is None:
            # try matching simple variants
            for i,h in enumerate(normed):
                if h == col.upper():
                    found = i
                    break
        mapping[col] = found
    return mapping


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--indir', default='work1/party-admin/seed/06_seed_census_mesh_2020/census_mesh_2020_data', help='Input directory with CSV files')
    # removed --outfile option per request; single-file output will use default path below
    parser.add_argument('--outdir', default='C:/Users/minamide/workspace/cloudflear/d1_project/party-admin-api/work1/party-admin/seed/06_seed_census_mesh_2020/SQL', help='Output directory for split SQL files')
    parser.add_argument('--chunk-size', type=int, default=1000, help='Number of INSERTs per output file')
    args = parser.parse_args()

    cols_sql = ','.join(TARGET_COLS)

    input_dir = os.path.expanduser(args.indir)
    csv_files = sorted(glob.glob(os.path.join(input_dir, '*')))
    if not csv_files:
        print(f'No files found in {input_dir}', file=sys.stderr)
        return

    output_dir = os.path.expanduser(args.outdir) if args.outdir else None
    chunk_size = int(args.chunk_size or 1000)

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    # default single-file output path (used when outdir is not provided)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    default_output_file = os.path.join(script_dir, 'SQL', '06_seed_census_mesh_2020.sql')
    if not output_dir:
        os.makedirs(os.path.dirname(default_output_file), exist_ok=True)

    def open_new_file(part_index):
        filename = f'seed_census_mesh_2020_part_{str(part_index).zfill(4)}.sql'
        full_path = os.path.join(output_dir, filename) if output_dir else default_output_file
        fh = open(full_path, 'w', encoding='utf-8')
        fh.write('-- Generated by genarate_census_mesh_2020.py\n')
        fh.write('-- Input dir: ' + input_dir + '\n\n')
        return fh, full_path

    part_index = 1
    part_count = 0
    outfile_handle = None
    output_file_path = None

    # open first file
    if output_dir:
        outfile_handle, output_file_path = open_new_file(part_index)
    else:
        outfile_handle = open(default_output_file, 'w', encoding='utf-8')
        outfile_handle.write('-- Generated by genarate_census_mesh_2020.py\n')
        outfile_handle.write('-- Input dir: ' + input_dir + '\n\n')

    for csv_path in csv_files:
            # skip non-files
            if not os.path.isfile(csv_path):
                continue
            try:
                # try multiple encodings (utf-8-sig, cp932, euc_jp, ...)
                encodings_to_try = ['utf-8-sig','utf-8','cp932','shift_jis','euc_jp','iso-2022-jp','latin1']
                text_io = None
                with open(csv_path, 'rb') as binary_file:
                    data = binary_file.read()
                for enc in encodings_to_try:
                    try:
                        text = data.decode(enc)
                        text_io = io.StringIO(text)
                        break
                    except Exception:
                        text_io = None
                if text_io is None:
                    print(f'Error processing {csv_path}: unable to decode file with tried encodings', file=sys.stderr)
                    continue

                reader = csv.reader(text_io)
                try:
                    header = next(reader)
                except StopIteration:
                    continue
                column_index_map = build_index_map(header)

                outfile_handle.write(f'-- source: {csv_path}\n')

                for csv_row in reader:
                        key_index = column_index_map.get('key_code')
                        key_value = None
                        if key_index is None or key_index >= len(csv_row):
                            key_value = csv_row[0].strip() if len(csv_row) > 0 else ''
                        else:
                            key_value = csv_row[key_index].strip()
                        if key_value == '':
                            continue

                        values_list = []
                        for column in TARGET_COLS:
                            idx = column_index_map.get(column)
                            raw = None
                            if idx is not None and idx < len(csv_row):
                                raw = csv_row[idx]
                            else:
                                pos = TARGET_COLS.index(column)
                                if pos < len(csv_row):
                                    raw = csv_row[pos]
                            values_list.append(value_to_sql(column, raw))

                        values_sql = ','.join(values_list)
                        key_sql = values_list[0]
                        outfile_handle.write(f'INSERT INTO census_mesh_2020 ({cols_sql}) SELECT {values_sql} WHERE NOT EXISTS (SELECT 1 FROM census_mesh_2020 WHERE key_code = {key_sql});\n')
                        part_count += 1
                        # rotate files when reaching chunk size
                        if output_dir and part_count >= chunk_size:
                            outfile_handle.close()
                            part_index += 1
                            part_count = 0
                            outfile_handle, output_file_path = open_new_file(part_index)
            except Exception as e:
                print(f'Error processing {csv_path}: {e}', file=sys.stderr)
                continue
    # close final handle
    try:
        if out:
            out.close()
    except Exception:
        pass


if __name__ == '__main__':
    main()
